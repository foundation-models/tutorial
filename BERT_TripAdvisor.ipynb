{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TripAdvisor",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossein20s/tutorial/blob/master/BERT_TripAdvisor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ILazuZbQmMT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First be sure you have connection to Google Drive and Cloud Storage to read data, python files and model"
      ]
    },
    {
      "metadata": {
        "id": "WqJLPl2M_Ybg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AJskoZY_iO6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilNWtD4Kck2j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "SRC_DIR='/content/gdrive/My Drive/src/'\n",
        "REPO_DIR=SRC_DIR +  '/bert_repo/'\n",
        "SRC_DIR_SHELL='/content/gdrive/My\\ Drive/src/'\n",
        "\n",
        "!mkdir $SRC_DIR_SHELL\n",
        "!cd $SRC_DIR_SHELL; git clone https://github.com/google-research/bert bert_repo\n",
        "if not REPO_DIR in sys.path:\n",
        "  sys.path.append(REPO_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYZhJtIOlyAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TASK = 'COLA'\n",
        "\n",
        "from google.colab import auth\n",
        "import pprint\n",
        "import json\n",
        "\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "  \n",
        "  \n",
        "BUCKET = 'medicalblockchain_dev' #@param {type:\"string\"}\n",
        "MODEL_INPUT_DIR = 'epoc3.0L12Out' #@param {type:\"string\"}\n",
        "MODEL_OUT_DIR = 'out' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "#OUTPUT_DIR_TFHUB = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, MODEL_OUT_DIR)\n",
        "MODEL_INPUT_DIR = 'gs://{}/bert-checkpoints/models/{}'.format(BUCKET, MODEL_INPUT_DIR)\n",
        "MODEL_OUTPUT_DIR = 'gs://{}/bert-checkpoints/models/{}'.format(BUCKET, MODEL_OUT_DIR)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4kQKn5Ol3Jg",
        "colab_type": "code",
        "outputId": "17b29ae4-0f3c-4faf-eb16-b47ea292bc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# from https://www.kaggle.com/madhab/jobposts & https://photos.app.goo.gl/Tqz5jvH8uhsMY94KA\n",
        "#TRAIN_FILE = 'data job posts.csv.gz'\n",
        "\n",
        "DATA_DIR = '/content/gdrive/My Drive/data/'\n",
        "# https://appliedmachinelearning.blog/2017/12/21/predict-the-happiness-on-tripadvisor-reviews-using-dense-neural-network-with-keras-hackerearth-challenge/\n",
        "DATA_FILE = 'trip_advisor_hackerearth_data.train.csv.gz'\n",
        "\n",
        "data = pd.read_csv(DATA_DIR + DATA_FILE, compression='gzip')\n",
        "                   #, skiprows=[0], header=None)\n",
        "\n",
        "print(data[:3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   User_ID                                        Description  \\\n",
            "0  id10326  The room was kind of clean but had a VERY stro...   \n",
            "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
            "2  id10328  I booked this hotel through Hotwire at the low...   \n",
            "\n",
            "        Browser_Used Device_Used Is_Response  \n",
            "0               Edge      Mobile   not happy  \n",
            "1  Internet Explorer      Mobile   not happy  \n",
            "2            Mozilla      Tablet   not happy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z9o-w6gbpG-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "********************************************\n",
        "You can jump on loading model from checkpint\n",
        "*********************************************"
      ]
    },
    {
      "metadata": {
        "id": "6DhT468nNOGZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "\n",
        "label = lb.fit_transform(data['Is_Response'])\n",
        "text = data['Description'].replace(r'\\n',' ',regex=True)\n",
        "\n",
        "df_bert = pd.DataFrame({'user_id':data['User_ID'], 'label':label, 'alpha':['a']*data.shape[0], 'text':text})\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GtbhWaZTTq6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_FILE = 'trip_advisor_hackerearth_data.test.csv.gz'\n",
        "\n",
        "df_test = pd.read_csv(DATA_DIR + DATA_FILE, compression='gzip')\n",
        "\n",
        "df_bert_test = pd.DataFrame({'User_ID':df_test['User_ID'],\n",
        "                 'text':df_test['Description'].replace(r'\\n',' ',regex=True)})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ueFQEzD5Uc3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Saving dataframes to .tsv format as required by BERT\n",
        "df_bert_train.to_csv(DATA_DIR + '/train.tsv', sep='\\t', index=False, header=False)\n",
        "df_bert_dev.to_csv(DATA_DIR + '/dev.tsv', sep='\\t', index=False, header=False)\n",
        "df_bert_test.to_csv(DATA_DIR + 'test.tsv', sep='\\t', index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHqf0fk7Vwze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*******************\n",
        "Loading model from Checkpoint\n",
        "********************"
      ]
    },
    {
      "metadata": {
        "id": "2hUYC6MBmphW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*******  From here you need to run to load model *******"
      ]
    },
    {
      "metadata": {
        "id": "8lDl1WSHpe_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "FWQfQdFCI_TP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb042e46-a10f-4a23-98de-fbf40cbd8f5f"
      },
      "cell_type": "code",
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL \n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EJ0NlkZbrIPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "SAVE_CHECKPOINTS_STEPS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yijUoofqq8tK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import run_classifier\n",
        "\n",
        "processors = {\n",
        "  \"cola\": run_classifier.ColaProcessor,\n",
        "  \"mnli\": run_classifier.MnliProcessor,\n",
        "  \"mrpc\": run_classifier.MrpcProcessor,\n",
        "}\n",
        "processor = processors[TASK.lower()]()\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "# Compute number of train and warmup steps from batch size\n",
        "train_examples = processor.get_train_examples(DATA_DIR)\n",
        "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4Iee4_drsqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6epz4_Uq82I",
        "colab_type": "code",
        "outputId": "dfe14167-f1f7-4b52-bf04-bbaf35338121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#OUTPUT_DIR = OUTPUT_DIR_TFHUB.replace('bert-tfhub', 'bert-checkpoints')\n",
        "\n",
        "tf.gfile.MakeDirs(MODEL_OUTPUT_DIR)\n",
        "INIT_CHECKPOINT=MODEL_INPUT_DIR\n",
        "#INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "\n",
        "import modeling\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "import tokenization\n",
        "import run_classifier_with_tfhub\n",
        "#tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True) # causing problem\n",
        "\n",
        "print('loading model from checkpoint directoryy {}', INIT_CHECKPOINT)\n",
        "\n",
        "import run_classifier\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "#import run_pretraining\n",
        "#model_fn = run_pretraining.model_fn_builder(\n",
        "  bert_config=bert_config,\n",
        "  init_checkpoint=INIT_CHECKPOINT,\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  use_one_hot_embeddings=True)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model from checkpoint directoryy {} gs://medicalblockchain_dev/bert-checkpoints/models/epoc3.0L12Out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D2785Fv_z-T-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*****************************\n",
        "You can jump on evaluating the model\n",
        "********************************\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V0TTzDIRwmJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "\n",
        "\n",
        "#os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "#import run_classifier_with_tfhub\n",
        "#tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
        "\n",
        "#model_fn = run_classifier_with_tfhub.model_fn_builder(\n",
        "#  num_labels=len(label_list),\n",
        "#  learning_rate=LEARNING_RATE,\n",
        "#  num_train_steps=num_train_steps,\n",
        "#  num_warmup_steps=num_warmup_steps,\n",
        "#  use_tpu=True,\n",
        "#  bert_hub_module_handle=BERT_MODEL_HUB\n",
        "#)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kCs3myAxwx7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import run_classifier\n",
        "\n",
        "MAX_SEQ_LENGTH = 400\n",
        "\n",
        "# Train the model\n",
        "def model_train(estimator):\n",
        "  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_examples)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n",
        "\n",
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "  eval_examples = processor.get_dev_examples(DATA_DIR)\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(DATA_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "      \n",
        "def model_predict(estimator):\n",
        "  # Make predictions on a subset of eval examples\n",
        "  prediction_examples = processor.get_dev_examples(DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "  input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "#  output_test_file = os.path.join(OUTPUT_DIR, \"test_results.txt\")\n",
        "#  with tf.gfile.GFile(output_test_file, \"w\") as writer:\n",
        "#    print(\"***** Test results *****\")\n",
        "#    for key in sorted(predictions.keys()):\n",
        "#      print('  {} = {}'.format(key, str(predictions[key])))\n",
        "#      writer.write(\"%s = %s\\n\" % (key, str(predictions[key])))\n",
        "\n",
        "  for example, prediction in zip(prediction_examples, predictions):\n",
        "    print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\n' % (example.text_a, example.text_b, str(example.label), prediction['probabilities']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lAc5HAWuoJGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(MODEL_OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oSPn4j4wx6xA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_train(estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zvqu41BS0ik4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "************************************\n",
        "Evaluating the model\n",
        "***************************"
      ]
    },
    {
      "metadata": {
        "id": "C4tGns0q99b3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "14 minutes for EPICS 3\n",
        "\n",
        "2nd time takes half an hour"
      ]
    },
    {
      "metadata": {
        "id": "jhPPQdoO0Uev",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_eval(estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2Z729htHCYJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_predict(estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hK-4KS9XOmLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With 1 Epoch this is the eval result from tfhub\n",
        "\n",
        "* eval_accuracy = 0.8932292\n",
        "* eval_loss = 0.5588442\n",
        "* global_step = 4817\n",
        "* loss = 0.60744804\n",
        "\n",
        "with 3 epoc\n",
        "\n",
        "  * eval_accuracy = 0.9166667\n",
        "  * eval_loss = 0.43633988\n",
        "  * global_step = 4817\n",
        "  * loss = 0.34420708\n",
        "\n",
        "\n",
        "from checkpoint\n",
        "\n",
        " *  eval_accuracy = 0.9010417\n",
        " *  eval_loss = 0.4951627\n",
        " *  global_step = 4817\n",
        " *  loss = 0.4208469\n",
        "\n",
        "Finally worked from gs://medicalblockchain_dev/bert-tfhub/models/test  \n",
        "  * eval_accuracy = 0.6744792\n",
        "  * eval_loss = 0.68638927\n",
        "  * global_step = 0\n",
        "  * loss = 0.68546134\n",
        "  \n",
        "On another run\n",
        "  * eval_accuracy = 0.5703125\n",
        "  * eval_loss = 0.6917891\n",
        "  * global_step = 0\n",
        "  * loss = 0.6918879\n",
        "  \n",
        "From gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt without training\n",
        "exactly same of tfhub\n",
        "  * eval_accuracy = 0.328125\n",
        "  * eval_loss = 0.78027385\n",
        "  * global_step = 0\n",
        "  * loss = 0.7998459"
      ]
    },
    {
      "metadata": {
        "id": "22MNg-QiK8NB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd '/content/gdrive/My Drive/src/bert_repo'; python run_classifier.py \\\n",
        "--task_name=cola \\\n",
        "--do_train=true \\\n",
        "--do_eval=true \\\n",
        "--do_predict=true \\\n",
        "--data_dir='/content/gdrive/My Drive/data/' \\\n",
        "--vocab_file='gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt' \\\n",
        "--bert_config_file='gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_config.json' \\\n",
        "--init_checkpoint='gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt' \\\n",
        "--max_seq_length=400 \\\n",
        "--train_batch_size=8 \\\n",
        "--learning_rate=2e-5 \\\n",
        "--num_train_epochs=3.0 \\\n",
        "--output_dir='gs://medicalblockchain_dev/bert-checkpoints/models/COLA' \\\n",
        "--do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hB2bYbtZXtmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from importlib import reload  # Python 3.4+ only.\n",
        "import modeling\n",
        "reload(modeling)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qR9P5AwAJUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil mv gs://medicalblockchain_dev/bert-checkpoints/models/test gs://medicalblockchain_dev/bert-checkpoints/models/epoc3L12"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}